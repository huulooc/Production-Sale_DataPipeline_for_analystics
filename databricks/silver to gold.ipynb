{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8780b42c-4f3e-4196-906b-3a1ecbefa0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Doing transformation for all other tables [Changing Column names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a87dd2-3b23-4e99-8319-c074c818792a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.awssaledatalake.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"3fd7ad3c-3291-46b6-9ce0-c360057c95c1\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"ax48Q~gLSH3ZZDSqSwqsi8k148.rPG8lytSSpcJT\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/07acb355-56bc-489b-b98c-8fea440460e8/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e63414-0aea-45d2-be41-00f74e5179b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'CountryRegionCurrency' vào sliver_dfs['CountryRegionCurrency']\n✅ Loaded bảng 'CreditCard' vào sliver_dfs['CreditCard']\n✅ Loaded bảng 'Currency' vào sliver_dfs['Currency']\n✅ Loaded bảng 'CurrencyRate' vào sliver_dfs['CurrencyRate']\n✅ Loaded bảng 'Customer' vào sliver_dfs['Customer']\n✅ Loaded bảng 'PersonCreditCard' vào sliver_dfs['PersonCreditCard']\n✅ Loaded bảng 'SalesOrderDetail' vào sliver_dfs['SalesOrderDetail']\n✅ Loaded bảng 'SalesOrderHeader' vào sliver_dfs['SalesOrderHeader']\n✅ Loaded bảng 'SalesOrderHeaderSalesReason' vào sliver_dfs['SalesOrderHeaderSalesReason']\n✅ Loaded bảng 'SalesPerson' vào sliver_dfs['SalesPerson']\n✅ Loaded bảng 'SalesPersonQuotaHistory' vào sliver_dfs['SalesPersonQuotaHistory']\n✅ Loaded bảng 'SalesReason' vào sliver_dfs['SalesReason']\n✅ Loaded bảng 'SalesTaxRate' vào sliver_dfs['SalesTaxRate']\n✅ Loaded bảng 'SalesTerritory' vào sliver_dfs['SalesTerritory']\n✅ Loaded bảng 'SalesTerritoryHistory' vào sliver_dfs['SalesTerritoryHistory']\n✅ Loaded bảng 'ShoppingCartItem' vào sliver_dfs['ShoppingCartItem']\n✅ Loaded bảng 'SpecialOffer' vào sliver_dfs['SpecialOffer']\n✅ Loaded bảng 'SpecialOfferProduct' vào sliver_dfs['SpecialOfferProduct']\n✅ Loaded bảng 'Store' vào sliver_dfs['Store']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "silver_path = \"abfss://sliver@awssaledatalake.dfs.core.windows.net/Sales/\"\n",
    "\n",
    "folders = dbutils.fs.ls(silver_path)\n",
    "\n",
    "\n",
    "sliver_dfs = {}\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\") \n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        sliver_dfs[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"load faill'{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581a7697-ceb5-49fa-9771-3a1bc80fde9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Processing CountryRegionCurrency ...\n✅ Saved CountryRegionCurrency → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/CountryRegionCurrency/\n\uD83D\uDD04 Processing CreditCard ...\n✅ Saved CreditCard → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/CreditCard/\n\uD83D\uDD04 Processing Currency ...\n✅ Saved Currency → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/Currency/\n\uD83D\uDD04 Processing CurrencyRate ...\n✅ Saved CurrencyRate → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/CurrencyRate/\n\uD83D\uDD04 Processing Customer ...\n✅ Saved Customer → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/Customer/\n\uD83D\uDD04 Processing PersonCreditCard ...\n✅ Saved PersonCreditCard → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/PersonCreditCard/\n\uD83D\uDD04 Processing SalesOrderDetail ...\n✅ Saved SalesOrderDetail → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesOrderDetail/\n\uD83D\uDD04 Processing SalesOrderHeader ...\n✅ Saved SalesOrderHeader → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesOrderHeader/\n\uD83D\uDD04 Processing SalesOrderHeaderSalesReason ...\n✅ Saved SalesOrderHeaderSalesReason → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesOrderHeaderSalesReason/\n\uD83D\uDD04 Processing SalesPerson ...\n✅ Saved SalesPerson → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesPerson/\n\uD83D\uDD04 Processing SalesPersonQuotaHistory ...\n✅ Saved SalesPersonQuotaHistory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesPersonQuotaHistory/\n\uD83D\uDD04 Processing SalesReason ...\n✅ Saved SalesReason → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesReason/\n\uD83D\uDD04 Processing SalesTaxRate ...\n✅ Saved SalesTaxRate → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesTaxRate/\n\uD83D\uDD04 Processing SalesTerritory ...\n✅ Saved SalesTerritory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesTerritory/\n\uD83D\uDD04 Processing SalesTerritoryHistory ...\n✅ Saved SalesTerritoryHistory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SalesTerritoryHistory/\n\uD83D\uDD04 Processing ShoppingCartItem ...\n✅ Saved ShoppingCartItem → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/ShoppingCartItem/\n\uD83D\uDD04 Processing SpecialOffer ...\n✅ Saved SpecialOffer → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SpecialOffer/\n\uD83D\uDD04 Processing SpecialOfferProduct ...\n✅ Saved SpecialOfferProduct → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/SpecialOfferProduct/\n\uD83D\uDD04 Processing Store ...\n✅ Saved Store → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/Store/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, when\n",
    "\n",
    "# Gold path\n",
    "gold_path = \"abfss://gold@awssaledatalake.dfs.core.windows.net/Sales/\"\n",
    "\n",
    "# Dict to store Gold DataFrames\n",
    "gold_dfs = {}\n",
    "\n",
    "for name, df in sliver_dfs.items():\n",
    "    print(f\"Processing {name} ...\")\n",
    "    \n",
    "    # 1. Standardize string columns (trim + uppercase for codes)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "            if \"code\" in c.lower():\n",
    "                df = df.withColumn(c, upper(col(c)))\n",
    "    \n",
    "    # 2. Fill null values (N/A for strings, 0 for numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, when(col(c).isNull(), \"N/A\").otherwise(col(c)))\n",
    "        elif t in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.withColumn(c, when(col(c).isNull(), 0).otherwise(col(c)))\n",
    "    \n",
    "    # 3. Apply business rules\n",
    "    if name.lower() == \"customer\" and \"phone_number\" in df.columns:\n",
    "        df = df.withColumn(\"phone_number\", trim(col(\"phone_number\")))\n",
    "    \n",
    "    if name.lower() == \"salesorderheader\":\n",
    "        if set([\"SubTotal\", \"TaxAmt\", \"Freight\"]).issubset(set(df.columns)):\n",
    "            df = df.withColumn(\"total_amount\", col(\"SubTotal\") + col(\"TaxAmt\") + col(\"Freight\"))\n",
    "    \n",
    "    # 4. Save into Gold (Delta format)\n",
    "    output_path = f\"{gold_path}{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "    \n",
    "    gold_dfs[name] = df\n",
    "    print(f\" Saved {name} → Gold: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea23d10-7023-469d-a631-1ae738d698b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'BillOfMaterials' vào pro_sliver_dfs['BillOfMaterials']\n✅ Loaded bảng 'Location' vào pro_sliver_dfs['Location']\n✅ Loaded bảng 'Product' vào pro_sliver_dfs['Product']\n✅ Loaded bảng 'ProductCategory' vào pro_sliver_dfs['ProductCategory']\n✅ Loaded bảng 'ProductCostHistory' vào pro_sliver_dfs['ProductCostHistory']\n✅ Loaded bảng 'ProductSubcategory' vào pro_sliver_dfs['ProductSubcategory']\n✅ Loaded bảng 'ScrapReason' vào pro_sliver_dfs['ScrapReason']\n✅ Loaded bảng 'TransactionHistory' vào pro_sliver_dfs['TransactionHistory']\n✅ Loaded bảng 'TransactionHistoryArchive' vào pro_sliver_dfs['TransactionHistoryArchive']\n✅ Loaded bảng 'UnitMeasure' vào pro_sliver_dfs['UnitMeasure']\n✅ Loaded bảng 'WorkOrder' vào pro_sliver_dfs['WorkOrder']\n✅ Loaded bảng 'WorkOrderRouting' vào pro_sliver_dfs['WorkOrderRouting']\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn Silver\n",
    "silver_path = \"abfss://sliver@awssaledatalake.dfs.core.windows.net/Production/\"\n",
    "\n",
    "folders = dbutils.fs.ls(silver_path)\n",
    "\n",
    "pro_sliver_dfs = {}\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\")  \n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        pro_sliver_dfs[table_name] = df\n",
    "        \n",
    "        print(f\"load faill '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26a25dd-a9b0-4d3b-81ad-7268d122ac15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Processing BillOfMaterials ...\n✅ Saved BillOfMaterials → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/BillOfMaterials/\n\uD83D\uDD04 Processing Location ...\n✅ Saved Location → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/Location/\n\uD83D\uDD04 Processing Product ...\n✅ Saved Product → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/Product/\n\uD83D\uDD04 Processing ProductCategory ...\n✅ Saved ProductCategory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/ProductCategory/\n\uD83D\uDD04 Processing ProductCostHistory ...\n✅ Saved ProductCostHistory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/ProductCostHistory/\n\uD83D\uDD04 Processing ProductSubcategory ...\n✅ Saved ProductSubcategory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/ProductSubcategory/\n\uD83D\uDD04 Processing ScrapReason ...\n✅ Saved ScrapReason → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/ScrapReason/\n\uD83D\uDD04 Processing TransactionHistory ...\n✅ Saved TransactionHistory → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/TransactionHistory/\n\uD83D\uDD04 Processing TransactionHistoryArchive ...\n✅ Saved TransactionHistoryArchive → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/TransactionHistoryArchive/\n\uD83D\uDD04 Processing UnitMeasure ...\n✅ Saved UnitMeasure → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/UnitMeasure/\n\uD83D\uDD04 Processing WorkOrder ...\n✅ Saved WorkOrder → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/WorkOrder/\n\uD83D\uDD04 Processing WorkOrderRouting ...\n✅ Saved WorkOrderRouting → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Production/WorkOrderRouting/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, when\n",
    "\n",
    "# Gold path\n",
    "gold_path = \"abfss://gold@awssaledatalake.dfs.core.windows.net/Production/\"\n",
    "\n",
    "# Dict to store Gold DataFrames\n",
    "pro_gold_dfs = {}\n",
    "\n",
    "for name, df in pro_sliver_dfs.items():\n",
    "    print(f\"Processing {name} ...\")\n",
    "    \n",
    "    # 1. Standardize string columns (trim + uppercase for codes)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "            if \"code\" in c.lower():\n",
    "                df = df.withColumn(c, upper(col(c)))\n",
    "    \n",
    "    # 2. Fill null values (N/A for strings, 0 for numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, when(col(c).isNull(), \"N/A\").otherwise(col(c)))\n",
    "        elif t in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.withColumn(c, when(col(c).isNull(), 0).otherwise(col(c)))\n",
    "    \n",
    "\n",
    "    \n",
    "    # 4. Save into Gold (Delta format)\n",
    "    output_path = f\"{gold_path}{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "    \n",
    "    pro_gold_dfs[name] = df\n",
    "    print(f\"Saved {name} → Gold: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e1714f-85e3-403f-aa33-25365889ff77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'CountryRegion' vào per_sliver_dfs['CountryRegion']\n✅ Loaded bảng 'Person' vào per_sliver_dfs['Person']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "silver_path = \"abfss://sliver@awssaledatalake.dfs.core.windows.net/Person/\"\n",
    "\n",
    "folders = dbutils.fs.ls(silver_path)\n",
    "\n",
    "per_sliver_dfs = {}\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\") \n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        per_sliver_dfs[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Load faill '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78722c29-fc31-4659-9723-d6f9efc618f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Processing CountryRegion ...\n✅ Saved CountryRegion → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Person/CountryRegion/\n\uD83D\uDD04 Processing Person ...\n✅ Saved Person → Gold: abfss://gold@awssaledatalake.dfs.core.windows.net/Person/Person/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, when\n",
    "\n",
    "# Gold path\n",
    "gold_path = \"abfss://gold@awssaledatalake.dfs.core.windows.net/Person/\"\n",
    "\n",
    "# Dict to store Gold DataFrames\n",
    "per_gold_dfs = {}\n",
    "\n",
    "for name, df in per_sliver_dfs.items():\n",
    "    print(f\"Processing {name} ...\")\n",
    "    \n",
    "    # 1. Standardize string columns (trim + uppercase for codes)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "            if \"code\" in c.lower():\n",
    "                df = df.withColumn(c, upper(col(c)))\n",
    "    \n",
    "    # 2. Fill null values (N/A for strings, 0 for numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, when(col(c).isNull(), \"N/A\").otherwise(col(c)))\n",
    "        elif t in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.withColumn(c, when(col(c).isNull(), 0).otherwise(col(c)))\n",
    "    \n",
    "    # 3. Apply business rules\n",
    "    if name.lower() == \"customer\" and \"phone_number\" in df.columns:\n",
    "        df = df.withColumn(\"phone_number\", trim(col(\"phone_number\")))\n",
    "    \n",
    "    if name.lower() == \"salesorderheader\":\n",
    "        if set([\"SubTotal\", \"TaxAmt\", \"Freight\"]).issubset(set(df.columns)):\n",
    "            df = df.withColumn(\"total_amount\", col(\"SubTotal\") + col(\"TaxAmt\") + col(\"Freight\"))\n",
    "    \n",
    "    # 4. Save into Gold (Delta format)\n",
    "    output_path = f\"{gold_path}{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "    \n",
    "    per_gold_dfs[name] = df\n",
    "    print(f\"Saved {name} → Gold: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver to gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}