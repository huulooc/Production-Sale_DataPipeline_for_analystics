{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28d491d-2622-4c0e-9410-b271a4f30f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Doing Transformation for all the tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc8e3ea-2766-4586-8368-f324c2e1e868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.awssaledatalake.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"3fd7ad3c-3291-46b6-9ce0-c360057c95c1\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"ax48Q~gLSH3ZZDSqSwqsi8k148.rPG8lytSSpcJT\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.awssaledatalake.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/07acb355-56bc-489b-b98c-8fea440460e8/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f979a2e-512f-447a-ba5e-0c759e28fffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'CountryRegionCurrency' vào sales_dfs['CountryRegionCurrency']\n✅ Loaded bảng 'CreditCard' vào sales_dfs['CreditCard']\n✅ Loaded bảng 'Currency' vào sales_dfs['Currency']\n✅ Loaded bảng 'CurrencyRate' vào sales_dfs['CurrencyRate']\n✅ Loaded bảng 'Customer' vào sales_dfs['Customer']\n✅ Loaded bảng 'PersonCreditCard' vào sales_dfs['PersonCreditCard']\n✅ Loaded bảng 'SalesOrderDetail' vào sales_dfs['SalesOrderDetail']\n✅ Loaded bảng 'SalesOrderHeader' vào sales_dfs['SalesOrderHeader']\n✅ Loaded bảng 'SalesOrderHeaderSalesReason' vào sales_dfs['SalesOrderHeaderSalesReason']\n✅ Loaded bảng 'SalesPerson' vào sales_dfs['SalesPerson']\n✅ Loaded bảng 'SalesPersonQuotaHistory' vào sales_dfs['SalesPersonQuotaHistory']\n✅ Loaded bảng 'SalesReason' vào sales_dfs['SalesReason']\n✅ Loaded bảng 'SalesTaxRate' vào sales_dfs['SalesTaxRate']\n✅ Loaded bảng 'SalesTerritory' vào sales_dfs['SalesTerritory']\n✅ Loaded bảng 'SalesTerritoryHistory' vào sales_dfs['SalesTerritoryHistory']\n✅ Loaded bảng 'ShoppingCartItem' vào sales_dfs['ShoppingCartItem']\n✅ Loaded bảng 'SpecialOffer' vào sales_dfs['SpecialOffer']\n✅ Loaded bảng 'SpecialOfferProduct' vào sales_dfs['SpecialOfferProduct']\n✅ Loaded bảng 'Store' vào sales_dfs['Store']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"abfss://brozen@awssaledatalake.dfs.core.windows.net/Sales/\"\n",
    "\n",
    "folders = dbutils.fs.ls(path)\n",
    "\n",
    "sales_dfs = {}\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\")\n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(path)\n",
    "        sales_dfs[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"load faill '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20161ef-10ee-418f-81d7-90de244470d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed and saved table CountryRegionCurrency into Silver\n✅ Processed and saved table CreditCard into Silver\n✅ Processed and saved table Currency into Silver\n✅ Processed and saved table CurrencyRate into Silver\n✅ Processed and saved table Customer into Silver\n✅ Processed and saved table PersonCreditCard into Silver\n✅ Processed and saved table SalesOrderDetail into Silver\n✅ Processed and saved table SalesOrderHeader into Silver\n✅ Processed and saved table SalesOrderHeaderSalesReason into Silver\n✅ Processed and saved table SalesPerson into Silver\n✅ Processed and saved table SalesPersonQuotaHistory into Silver\n✅ Processed and saved table SalesReason into Silver\n✅ Processed and saved table SalesTaxRate into Silver\n✅ Processed and saved table SalesTerritory into Silver\n✅ Processed and saved table SalesTerritoryHistory into Silver\n✅ Processed and saved table ShoppingCartItem into Silver\n✅ Processed and saved table SpecialOffer into Silver\n✅ Processed and saved table SpecialOfferProduct into Silver\n✅ Processed and saved table Store into Silver\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, trim\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for name in sales_dfs:\n",
    "    df = sales_dfs[name]  \n",
    "    \n",
    "    # 1. Normalize column names (CamelCase -> snake_case)\n",
    "    for old_col_name in df.columns:\n",
    "        new_col_name = \"\".join([\n",
    "            \"_\" + char if char.isupper() and not old_col_name[i-1].isupper() else char\n",
    "            for i, char in enumerate(old_col_name)\n",
    "        ]).lstrip(\"_\")\n",
    "        df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # 3. Standardize data types (dates, strings, numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        # Convert string columns that look like date/time\n",
    "        if t == \"string\" and (\"date\" in c.lower() or \"time\" in c.lower()):\n",
    "            df = df.withColumn(c, to_date(col(c), \"yyyy-MM-dd\"))\n",
    "        # Trim extra spaces in string columns\n",
    "        elif t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "\n",
    "    # 4. Handle null values \n",
    "\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == \"string\":\n",
    "            df = df.fillna({col_name: \"N/A\"})\n",
    "        elif dtype in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.fillna({col_name: 0})\n",
    "\n",
    "    # 5. Save to Silver (Delta format)\n",
    "    output_path = f\"abfss://sliver@awssaledatalake.dfs.core.windows.net/Sales/{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "\n",
    "    print(f\" Processed and saved table {name} into Silver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8635c76-7f78-4422-8e9b-d3065577a02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'BillOfMaterials' vào Production_dfs['BillOfMaterials']\n✅ Loaded bảng 'Location' vào Production_dfs['Location']\n✅ Loaded bảng 'Product' vào Production_dfs['Product']\n✅ Loaded bảng 'ProductCategory' vào Production_dfs['ProductCategory']\n✅ Loaded bảng 'ProductCostHistory' vào Production_dfs['ProductCostHistory']\n✅ Loaded bảng 'ProductSubcategory' vào Production_dfs['ProductSubcategory']\n✅ Loaded bảng 'ScrapReason' vào Production_dfs['ScrapReason']\n✅ Loaded bảng 'TransactionHistory' vào Production_dfs['TransactionHistory']\n✅ Loaded bảng 'TransactionHistoryArchive' vào Production_dfs['TransactionHistoryArchive']\n✅ Loaded bảng 'UnitMeasure' vào Production_dfs['UnitMeasure']\n✅ Loaded bảng 'WorkOrder' vào Production_dfs['WorkOrder']\n✅ Loaded bảng 'WorkOrderRouting' vào Production_dfs['WorkOrderRouting']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"abfss://brozen@awssaledatalake.dfs.core.windows.net/Production/\"\n",
    "\n",
    "\n",
    "folders = dbutils.fs.ls(path)\n",
    "\n",
    "Production_dfs = {}\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\")  \n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(path)\n",
    "        Production_dfs[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"load faill '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f76237a-ef7f-44b1-b3c7-f17e3fa9d225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed and saved table BillOfMaterials into Silver\n✅ Processed and saved table Location into Silver\n✅ Processed and saved table Product into Silver\n✅ Processed and saved table ProductCategory into Silver\n✅ Processed and saved table ProductCostHistory into Silver\n✅ Processed and saved table ProductSubcategory into Silver\n✅ Processed and saved table ScrapReason into Silver\n✅ Processed and saved table TransactionHistory into Silver\n✅ Processed and saved table TransactionHistoryArchive into Silver\n✅ Processed and saved table UnitMeasure into Silver\n✅ Processed and saved table WorkOrder into Silver\n✅ Processed and saved table WorkOrderRouting into Silver\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, trim\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for name in Production_dfs:\n",
    "    df = Production_dfs[name]   # Load each DataFrame from Bronze\n",
    "    \n",
    "    # 1. Normalize column names (CamelCase -> snake_case)\n",
    "    for old_col_name in df.columns:\n",
    "        new_col_name = \"\".join([\n",
    "            \"_\" + char if char.isupper() and not old_col_name[i-1].isupper() else char\n",
    "            for i, char in enumerate(old_col_name)\n",
    "        ]).lstrip(\"_\")\n",
    "        df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # 3. Standardize data types (dates, strings, numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        # Convert string columns that look like date/time\n",
    "        if t == \"string\" and (\"date\" in c.lower() or \"time\" in c.lower()):\n",
    "            df = df.withColumn(c, to_date(col(c), \"yyyy-MM-dd\"))\n",
    "        # Trim extra spaces in string columns\n",
    "        elif t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "\n",
    "    # 4. Handle null values \n",
    "\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == \"string\":\n",
    "            df = df.fillna({col_name: \"N/A\"})\n",
    "        elif dtype in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.fillna({col_name: 0})\n",
    "\n",
    "    # 5. Save to Silver (Delta format)\n",
    "    output_path = f\"abfss://sliver@awssaledatalake.dfs.core.windows.net/Production/{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "\n",
    "    print(f\" Processed and saved table {name} into Silver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f28dd4-8717-40b3-9f3d-6eae20e9f53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded bảng 'CountryRegion' vào Person_dfs['CountryRegion']\n✅ Loaded bảng 'Person' vào Person_dfs['Person']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"abfss://brozen@awssaledatalake.dfs.core.windows.net/Person/\"\n",
    "\n",
    "folders = dbutils.fs.ls(path)\n",
    "\n",
    "Person_dfs = {}\n",
    "\n",
    "for folder in folders:\n",
    "    table_name = folder.name.replace(\"/\", \"\")  \n",
    "    path = folder.path\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(path)\n",
    "        Person_dfs[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"load faill '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64a17c0-379f-4286-b3a3-f35888f96e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed and saved table CountryRegion into Silver\n✅ Processed and saved table Person into Silver\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, trim\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for name in Person_dfs:\n",
    "    df = Person_dfs[name]   # Load each DataFrame from Bronze\n",
    "    \n",
    "    # 1. Normalize column names (CamelCase -> snake_case)\n",
    "    for old_col_name in df.columns:\n",
    "        new_col_name = \"\".join([\n",
    "            \"_\" + char if char.isupper() and not old_col_name[i-1].isupper() else char\n",
    "            for i, char in enumerate(old_col_name)\n",
    "        ]).lstrip(\"_\")\n",
    "        df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # 3. Standardize data types (dates, strings, numbers)\n",
    "    for c, t in df.dtypes:\n",
    "        # Convert string columns that look like date/time\n",
    "        if t == \"string\" and (\"date\" in c.lower() or \"time\" in c.lower()):\n",
    "            df = df.withColumn(c, to_date(col(c), \"yyyy-MM-dd\"))\n",
    "        # Trim extra spaces in string columns\n",
    "        elif t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "\n",
    "    # 4. Handle null values \n",
    "\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == \"string\":\n",
    "            df = df.fillna({col_name: \"N/A\"})\n",
    "        elif dtype in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "            df = df.fillna({col_name: 0})\n",
    "\n",
    "    # 5. Save to Silver (Delta format)\n",
    "    output_path = f\"abfss://sliver@awssaledatalake.dfs.core.windows.net/Person/{name}/\"\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .save(output_path)\n",
    "    )\n",
    "\n",
    "    print(f\" Processed and saved table {name} into Silver\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}